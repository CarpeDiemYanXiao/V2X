# scripts/train_teacher_regression.py
"""
ç›´æ¥å›å½’åæ ‡çš„è®­ç»ƒæ–¹æ¡ˆ
ä¸ä½¿ç”¨åˆ†ç±»ï¼Œç›´æ¥é¢„æµ‹åæ ‡å€¼
å¯èƒ½æ¯”åˆ†ç±»æ–¹æ³•æ›´ç®€å•æœ‰æ•ˆ
"""
import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import logging
from PIL import Image
import numpy as np

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, ROOT)
sys.path.append("/root/autodl-tmp/models/florence2/Florence-2-base")
sys.path.append("/root/autodl-tmp/models/florence2/Florence-2-large")

from src.utils.config import load_config
from src.data.dataset import V2XVLMDataset
from src.models.v2x_vlm_teacher_regression import V2XVLMTeacherRegression

def collate_fn(batch):
    """æ‹¼æ¥è½¦è¾†-åŸºç¡€è®¾æ–½å›¾åƒ"""
    vehicle_imgs = [b["vehicle_img"] for b in batch]
    infra_imgs = [b["infra_img"] for b in batch]
    prompts = [b["prompt"] for b in batch]
    trajectories = torch.stack([b["trajectory"] for b in batch])

    combined_images = []
    for v_img, i_img, prompt in zip(vehicle_imgs, infra_imgs, prompts):
        if not prompt.strip():
            raise ValueError(f"Empty prompt found: {prompt}")
        
        if v_img.size != i_img.size:
            i_img = i_img.resize(v_img.size)
        
        combined_width = v_img.width + i_img.width
        combined_img = Image.new('RGB', (combined_width, v_img.height))
        combined_img.paste(v_img, (0, 0))
        combined_img.paste(i_img, (v_img.width, 0))
        
        combined_images.append(combined_img)

    return {
        "combined_images": combined_images,
        "prompts": prompts,
        "trajectory": trajectories
    }

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - [RegressionTrain] - %(message)s'
    )
    return logging.getLogger(__name__)

def main():
    cfg_path = os.path.join(ROOT, "configs", "config.yaml")
    cfg = load_config(cfg_path)
    logger = setup_logger()
    
    cfg.output_dir = os.path.join(cfg.output_dir, "teacher_training_regression")
    os.makedirs(cfg.output_dir, exist_ok=True)
    
    device = torch.device(cfg.device if torch.cuda.is_available() else "cpu")
    logger.info(f"è®­ç»ƒ Teacher æ¨¡å‹ï¼ˆç›´æ¥å›å½’ï¼‰| è®¾å¤‡: {device} | è¾“å‡º: {cfg.output_dir}")

    # æ•°æ®å‡†å¤‡
    train_dataset = V2XVLMDataset(split="train", cfg=cfg)
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=cfg.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True
    )

    val_dataset = V2XVLMDataset(split="val", cfg=cfg)
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )

    # åˆå§‹åŒ–æ¨¡å‹
    logger.info("åˆå§‹åŒ– Teacher æ¨¡å‹ (Florence-2-large, Regression)...")
    teacher = V2XVLMTeacherRegression(cfg).to(device)

    # é…ç½®å‚æ•°å†»ç»“ç­–ç•¥ï¼šé»˜è®¤å®Œå…¨å†»ç»“backboneï¼ˆæ›´ç¨³å®šï¼‰
    freeze_backbone = os.getenv("FREEZE_BACKBONE", "true").lower() == "true"
    
    head_params = []
    backbone_params = []
    
    for name, param in teacher.named_parameters():
        # A. è§†è§‰éƒ¨åˆ†ï¼šæ°¸è¿œå†»ç»“
        if "vision" in name.lower() or "davit" in name.lower():
            param.requires_grad = False
            
        # B. é¢„æµ‹å¤´ (Head)ï¼šå¿…é¡»è®­ç»ƒ
        elif "traj_head" in name or "feature_fusion" in name:
            param.requires_grad = True
            head_params.append(param)
            
        # C. è¯­è¨€æ¨¡å‹ (Backbone)ï¼šæ ¹æ®ç­–ç•¥å†³å®š
        else:
            if freeze_backbone:
                # å®Œå…¨å†»ç»“æ¨¡å¼ï¼šåªè®­ç»ƒé¢„æµ‹å¤´
                param.requires_grad = False
            else:
                # éƒ¨åˆ†å¾®è°ƒæ¨¡å¼ï¼šè§£å†»backboneï¼Œä½†ç”¨å°å­¦ä¹ ç‡
                param.requires_grad = True
                backbone_params.append(param)
    
    if freeze_backbone:
        logger.info("ğŸ”¥ ç­–ç•¥: å®Œå…¨å†»ç»“Backbone | åªè®­ç»ƒå›å½’å¤´")
    else:
        logger.info("ğŸ”¥ ç­–ç•¥: è§†è§‰å†»ç»“ | Backboneå¾®è°ƒ(5e-5) | Headå¿«è®­(1e-2)")

    trainable_params = sum(p.numel() for p in teacher.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in teacher.parameters())
    logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M / æ€»å‚æ•°: {total_params/1e6:.2f}M ({100*trainable_params/total_params:.2f}%)")

    # ä¼˜åŒ–å™¨ï¼šæ ¹æ®è®­ç»ƒç­–ç•¥é€‰æ‹©
    if freeze_backbone:
        # å®Œå…¨å†»ç»“æ¨¡å¼ï¼šåªä¼˜åŒ–é¢„æµ‹å¤´ï¼Œä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡è®©æ¨¡å‹å¿«é€Ÿå­¦ä¹ 
        optimizer = torch.optim.AdamW(
            head_params,
            lr=1e-2,  # æé«˜å­¦ä¹ ç‡ï¼Œè®©é¢„æµ‹èŒƒå›´å¿«é€Ÿæ‰©å¤§
            weight_decay=0.01
        )
        # ä½¿ç”¨æ›´æ¿€è¿›çš„å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=cfg.epochs,
            eta_min=1e-4  # æé«˜æœ€å°å­¦ä¹ ç‡
        )
        accumulation_steps = 1  # ä¸éœ€è¦æ¢¯åº¦ç´¯ç§¯
    else:
        # éƒ¨åˆ†å¾®è°ƒæ¨¡å¼ï¼šåˆ†åˆ«è®¾ç½®backboneå’Œheadçš„å­¦ä¹ ç‡
        optimizer = torch.optim.AdamW([
            {'params': backbone_params, 'lr': 5e-5},  # Backboneå­¦ä¹ ç‡æé«˜
            {'params': head_params, 'lr': 1e-2}       # Headå­¦ä¹ ç‡æé«˜
        ], weight_decay=0.01)
        
        # ä½¿ç”¨warmup + cosineè°ƒåº¦å™¨
        accumulation_steps = 4  # æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘æ˜¾å­˜å ç”¨
        num_actual_updates_per_epoch = len(train_loader) // accumulation_steps
        num_warmup_updates = min(10, num_actual_updates_per_epoch // 10)
        num_training_updates = num_actual_updates_per_epoch * cfg.epochs
        
        def lr_lambda(step):
            if step < num_warmup_updates:
                warmup_start = 0.1
                warmup_end = 1.0
                if num_warmup_updates > 0:
                    return warmup_start + (warmup_end - warmup_start) * (step + 1) / num_warmup_updates
                else:
                    return 1.0
            else:
                progress = (step - num_warmup_updates) / max(1, (num_training_updates - num_warmup_updates))
                return max(0.1, 0.5 * (1 + np.cos(np.pi * progress)))
        
        from torch.optim.lr_scheduler import LambdaLR
        scheduler = LambdaLR(optimizer, lr_lambda)
        
        logger.info(f"å­¦ä¹ ç‡è°ƒåº¦: Warmup {num_warmup_updates}æ­¥, æ€»æ›´æ–° {num_training_updates}æ­¥, æ¢¯åº¦ç´¯ç§¯ {accumulation_steps}æ­¥")
    
    best_val_l2 = float('inf')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(cfg.epochs):
        logger.info(f"Epoch {epoch + 1}/{cfg.epochs}")
        teacher.train()
        epoch_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
            combined_images = batch["combined_images"]
            prompts = batch["prompts"]
            
            gt_traj_coords = batch["trajectory"].to(device)  # [B, 45, 2]

            outputs = teacher(combined_images, prompts)
            pred_coords = outputs["trajectory_coords"]  # [B, 45, 2]
            
            # ä½¿ç”¨Huber Lossï¼ˆå¯¹å¼‚å¸¸å€¼é²æ£’ï¼Œç»“åˆäº†L1å’ŒL2çš„ä¼˜ç‚¹ï¼‰
            # delta=1.0: å½“è¯¯å·®<1ç±³æ—¶ç”¨L2ï¼Œè¯¯å·®>1ç±³æ—¶ç”¨L1
            loss = nn.functional.huber_loss(pred_coords, gt_traj_coords, reduction='mean', delta=10.0)
            
            # æ ¹æ®è®­ç»ƒç­–ç•¥é€‰æ‹©æ¢¯åº¦ç´¯ç§¯æ–¹å¼
            if freeze_backbone:
                # å®Œå…¨å†»ç»“æ¨¡å¼ï¼šæ¯ä¸ªbatchéƒ½æ›´æ–°
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(head_params, max_norm=1.0)
                optimizer.step()
                epoch_loss += loss.item()
            else:
                # éƒ¨åˆ†å¾®è°ƒæ¨¡å¼ï¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
                loss = loss / accumulation_steps
                loss.backward()
                
                if (batch_idx + 1) % accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(teacher.parameters(), max_norm=1.0)
                    optimizer.step()
                    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
                    optimizer.zero_grad()
                
                epoch_loss += loss.item() * accumulation_steps
            
            # Debugæ‰“å°ï¼šå¢åŠ æ›´å¤šä¿¡æ¯
            if batch_idx == 0:
                last_step_idx = 44
                pred_val = pred_coords[0, last_step_idx].detach().cpu().numpy()
                gt_val = gt_traj_coords[0, last_step_idx].cpu().numpy()
                error = np.linalg.norm(pred_val - gt_val)
                current_lr = optimizer.param_groups[0]['lr']
                
                # æ‰“å°æ•´æ¡è½¨è¿¹çš„ç»Ÿè®¡ä¿¡æ¯
                pred_all = pred_coords[0].detach().cpu().numpy()
                gt_all = gt_traj_coords[0].cpu().numpy()
                pred_range = f"[{pred_all.min():.1f}, {pred_all.max():.1f}]"
                gt_range = f"[{gt_all.min():.1f}, {gt_all.max():.1f}]"
                
                print(f"\n[Debug T=4.5s] GT: {gt_val} | Pred: {pred_val} | Error: {error:.2f}m | LR: {current_lr:.2e}")
                print(f"[Range] GT: {gt_range} | Pred: {pred_range} | Loss: {loss.item():.4f}")
        
        if freeze_backbone:
            scheduler.step()  # åœ¨epochç»“æŸæ—¶æ›´æ–°
            avg_loss = epoch_loss / len(train_loader)
            current_lr = optimizer.param_groups[0]['lr']
            logger.info(f"Train Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")
        else:
            avg_loss = epoch_loss / len(train_loader)
            backbone_lr = optimizer.param_groups[0]['lr']
            head_lr = optimizer.param_groups[1]['lr']
            logger.info(f"Train Loss: {avg_loss:.4f} | LR Backbone: {backbone_lr:.2e} | LR Head: {head_lr:.2e}")

        # éªŒè¯
        val_l2 = validate(teacher, val_loader, device)
        logger.info(f"Val L2 Error: {val_l2:.4f} meters")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_l2 < best_val_l2:
            best_val_l2 = val_l2
            save_path = os.path.join(cfg.output_dir, "teacher_best_regression.pth")
            torch.save(teacher.state_dict(), save_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹è‡³: {save_path}")

@torch.no_grad()
def validate(model, loader, device):
    model.eval()
    total_l2_error = 0.0
    total_valid_points = 0
    
    pbar = tqdm(loader, desc="Validating")
    
    for batch in pbar:
        combined_images = batch["combined_images"]
        prompts = batch["prompts"]
        
        gt_traj_coords = batch["trajectory"].to(device)

        outputs = model(combined_images, prompts)
        pred_coords = outputs["trajectory_coords"]
        
        # è®¡ç®—L2è·ç¦»
        distances = torch.norm(pred_coords - gt_traj_coords, dim=-1)  # [B, 45]
        
        total_l2_error += distances.sum().item()
        total_valid_points += distances.numel()
        
        pbar.set_postfix({"L2": f"{distances.mean().item():.2f}m"})
        
    avg_l2 = total_l2_error / total_valid_points if total_valid_points > 0 else 0.0
    return avg_l2

if __name__ == "__main__":
    main()
