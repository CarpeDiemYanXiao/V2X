# scripts/train_teacher_frozen.py
"""
å®Œå…¨å†»ç»“Backboneçš„è®­ç»ƒæ–¹æ¡ˆ
åªè®­ç»ƒé¢„æµ‹å¤´ï¼Œå°†Florence-2ä½œä¸ºç‰¹å¾æå–å™¨
ä¼˜ç‚¹ï¼šè®­ç»ƒç¨³å®šã€å¿«é€Ÿã€ä¸å®¹æ˜“è¿‡æ‹Ÿåˆ
ç¼ºç‚¹ï¼šå¯èƒ½æ€§èƒ½ä¸Šé™è¾ƒä½
"""
import os
import sys
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import logging
from PIL import Image
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, ROOT)
sys.path.append("/root/autodl-tmp/models/florence2/Florence-2-base")
sys.path.append("/root/autodl-tmp/models/florence2/Florence-2-large")

from src.utils.config import load_config
from src.data.dataset import V2XVLMDataset
from src.models.v2x_vlm_teacher import V2XVLMTeacher
from src.models.losses import trajectory_loss
from src.utils.tokenizer import TrajectoryTokenizer

def collate_fn(batch):
    """æ‹¼æ¥è½¦è¾†-åŸºç¡€è®¾æ–½å›¾åƒ"""
    vehicle_imgs = [b["vehicle_img"] for b in batch]
    infra_imgs = [b["infra_img"] for b in batch]
    prompts = [b["prompt"] for b in batch]
    trajectories = torch.stack([b["trajectory"] for b in batch])

    combined_images = []
    for v_img, i_img, prompt in zip(vehicle_imgs, infra_imgs, prompts):
        if not prompt.strip():
            raise ValueError(f"Empty prompt found: {prompt}")
        
        if v_img.size != i_img.size:
            i_img = i_img.resize(v_img.size)
        
        combined_width = v_img.width + i_img.width
        combined_img = Image.new('RGB', (combined_width, v_img.height))
        combined_img.paste(v_img, (0, 0))
        combined_img.paste(i_img, (v_img.width, 0))
        
        combined_images.append(combined_img)

    return {
        "combined_images": combined_images,
        "prompts": prompts,
        "trajectory": trajectories
    }

def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - [FrozenTrain] - %(message)s'
    )
    return logging.getLogger(__name__)

def main():
    cfg_path = os.path.join(ROOT, "configs", "config.yaml")
    cfg = load_config(cfg_path)
    logger = setup_logger()
    
    cfg.output_dir = os.path.join(cfg.output_dir, "teacher_training_frozen")
    os.makedirs(cfg.output_dir, exist_ok=True)
    
    device = torch.device(cfg.device if torch.cuda.is_available() else "cpu")
    logger.info(f"è®­ç»ƒ Teacher æ¨¡å‹ï¼ˆå®Œå…¨å†»ç»“Backboneï¼‰| è®¾å¤‡: {device} | è¾“å‡º: {cfg.output_dir}")

    tokenizer = TrajectoryTokenizer(cfg)

    # æ•°æ®å‡†å¤‡
    train_dataset = V2XVLMDataset(split="train", cfg=cfg)
    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=cfg.num_workers,
        collate_fn=collate_fn,
        pin_memory=True,
        drop_last=True
    )

    val_dataset = V2XVLMDataset(split="val", cfg=cfg)
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.batch_size,
        shuffle=False,
        num_workers=cfg.num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )

    # åˆå§‹åŒ–æ¨¡å‹
    logger.info("åˆå§‹åŒ– Teacher æ¨¡å‹ (Florence-2-large)...")
    teacher = V2XVLMTeacher(cfg).to(device)

    # ã€å…³é”®ã€‘å®Œå…¨å†»ç»“Backboneï¼Œåªè®­ç»ƒé¢„æµ‹å¤´
    logger.info("ğŸ”¥ ç­–ç•¥: å®Œå…¨å†»ç»“Backbone | åªè®­ç»ƒé¢„æµ‹å¤´")
    head_params = []
    
    for name, param in teacher.named_parameters():
        # åªè®­ç»ƒé¢„æµ‹å¤´ç›¸å…³çš„å‚æ•°
        if "traj_head" in name or "feature_fusion" in name:
            param.requires_grad = True
            head_params.append(param)
        else:
            # å†»ç»“æ‰€æœ‰å…¶ä»–å‚æ•°ï¼ˆåŒ…æ‹¬visionã€language modelç­‰ï¼‰
            param.requires_grad = False

    trainable_params = sum(p.numel() for p in teacher.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in teacher.parameters())
    logger.info(f"å¯è®­ç»ƒå‚æ•°: {trainable_params/1e6:.2f}M / æ€»å‚æ•°: {total_params/1e6:.2f}M ({100*trainable_params/total_params:.2f}%)")

    # åªä¼˜åŒ–é¢„æµ‹å¤´ï¼Œä½¿ç”¨è¾ƒå¤§çš„å­¦ä¹ ç‡
    optimizer = torch.optim.AdamW(
        head_params,
        lr=1e-3,  # åªè®­ç»ƒå¤´ï¼Œå¯ä»¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡
        weight_decay=0.01
    )

    # ç®€å•çš„å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=cfg.epochs,
        eta_min=1e-5
    )
    
    best_val_l2 = float('inf')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(cfg.epochs):
        logger.info(f"Epoch {epoch + 1}/{cfg.epochs}")
        teacher.train()
        epoch_loss = 0.0
        
        for batch_idx, batch in enumerate(tqdm(train_loader, desc="Training")):
            combined_images = batch["combined_images"]
            prompts = batch["prompts"]
            
            gt_traj_coords = batch["trajectory"].to(device)
            gt_tokens = tokenizer.coords_to_tokens(gt_traj_coords).to(device)
            
            # å¤„ç†padding
            gt_tokens[gt_tokens == 0] = -100 
            gt_tokens[gt_tokens == 1023] = -100

            outputs = teacher(combined_images, prompts)
            traj_logits = outputs["trajectory_logits"]
            
            loss = trajectory_loss(traj_logits, gt_tokens, label_smoothing=0.05)
            
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(head_params, max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            
            # Debugæ‰“å°
            if batch_idx == 0:
                last_step_idx = 44 
                reshaped_logits = traj_logits.view(-1, 45, 2, 1024)
                pred_probs = torch.softmax(reshaped_logits[0, last_step_idx, 0], dim=-1)
                max_prob, max_idx = torch.max(pred_probs, dim=-1)
                
                gt_val = gt_tokens[0, last_step_idx, 0].item()
                gt_display = "PADDING" if gt_val == -100 else gt_val
                current_lr = optimizer.param_groups[0]['lr']
                print(f"\n[Debug T=4.5s X-axis] GT Token: {gt_display} | Pred Max: {max_idx.item()} (Prob: {max_prob.item():.4f}) | LR: {current_lr:.2e}")
        
        scheduler.step()
        avg_loss = epoch_loss / len(train_loader)
        current_lr = optimizer.param_groups[0]['lr']
        logger.info(f"Train Loss: {avg_loss:.4f} | LR: {current_lr:.2e}")

        # éªŒè¯
        val_l2 = validate(teacher, val_loader, device, tokenizer)
        logger.info(f"Val L2 Error: {val_l2:.4f} meters")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_l2 < best_val_l2:
            best_val_l2 = val_l2
            save_path = os.path.join(cfg.output_dir, "teacher_best_frozen.pth")
            torch.save(teacher.state_dict(), save_path)
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹è‡³: {save_path}")

@torch.no_grad()
def validate(model, loader, device, tokenizer):
    model.eval()
    total_l2_error = 0.0
    total_valid_points = 0
    
    pbar = tqdm(loader, desc="Validating")
    
    for batch in pbar:
        combined_images = batch["combined_images"]
        prompts = batch["prompts"]
        
        gt_traj_coords = batch["trajectory"].to(device)
        gt_tokens = tokenizer.coords_to_tokens(gt_traj_coords).to(device)
        gt_tokens[gt_tokens == 0] = -100

        outputs = model(combined_images, prompts)
        traj_logits = outputs["trajectory_logits"]

        pred_token_ids = torch.argmax(traj_logits, dim=-1).view(-1, 45, 2)
        pred_coords = tokenizer.tokens_to_coords(pred_token_ids)
        
        mask = (gt_tokens != -100).all(dim=-1)
        distances = torch.norm(pred_coords - gt_traj_coords, dim=-1)
        valid_distances = distances[mask]
        
        if valid_distances.numel() > 0:
            total_l2_error += valid_distances.sum().item()
            total_valid_points += valid_distances.numel()
            pbar.set_postfix({"L2": f"{valid_distances.mean().item():.2f}m"})
        
    avg_l2 = total_l2_error / total_valid_points if total_valid_points > 0 else 0.0
    return avg_l2

if __name__ == "__main__":
    main()
