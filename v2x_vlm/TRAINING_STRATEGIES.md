# 训练策略说明

如果微调效果不理想，可以考虑以下几种训练策略：

## 策略对比

| 策略 | 可训练参数 | 训练速度 | 稳定性 | 性能上限 | 适用场景 |
|------|-----------|---------|--------|---------|---------|
| **完全冻结** | ~10M | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | 数据少、快速验证 |
| **部分微调** | ~500M | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | 数据充足、追求性能 |
| **全量微调** | ~1B+ | ⭐ | ⭐⭐ | ⭐⭐⭐⭐⭐ | 数据充足、资源充足 |

## 1. 完全冻结Backbone（推荐先试这个）

**优点：**
- ✅ 训练非常稳定，不容易过拟合
- ✅ 训练速度快，显存占用小
- ✅ 只需要训练预测头（~10M参数）
- ✅ 适合快速验证和调试

**缺点：**
- ❌ 性能上限可能较低
- ❌ 无法利用backbone的领域适应能力

### 使用方法

**方法1：使用专门的脚本**
```bash
python scripts/train_teacher_frozen.py
```

**方法2：使用环境变量**
```bash
FREEZE_BACKBONE=true python scripts/train_teacher.py
```

## 2. 部分微调（当前默认）

**优点：**
- ✅ 平衡了性能和稳定性
- ✅ 可以适应新领域但不会破坏预训练知识

**缺点：**
- ❌ 需要更多显存和计算资源
- ❌ 训练时间较长

### 使用方法
```bash
python scripts/train_teacher.py
```

## 3. 其他优化建议

### 如果训练效果仍不理想，可以尝试：

1. **检查数据质量**
   ```bash
   python scripts/check_data.py
   ```

2. **调整学习率**
   - 如果损失下降太慢：提高学习率（Head: 2e-3, Backbone: 5e-5）
   - 如果训练不稳定：降低学习率（Head: 5e-4, Backbone: 1e-5）

3. **减少梯度累积**
   - 将 `accumulation_steps` 从 8 降到 4，加快更新频率

4. **增加训练轮数**
   - 从 10 个 epoch 增加到 20-30 个

5. **使用更大的batch size**
   - 如果显存允许，增加 batch_size 到 4 或 8

## 推荐流程

1. **第一步：完全冻结训练**（快速验证）
   ```bash
   python scripts/train_teacher_frozen.py
   ```
   - 如果效果可以接受，就用这个方案
   - 如果效果太差，继续下一步

2. **第二步：部分微调**（追求性能）
   ```bash
   python scripts/train_teacher.py
   ```
   - 如果效果明显提升，就用这个方案
   - 如果提升不明显，考虑数据或模型架构问题

3. **第三步：调整超参数**
   - 根据训练日志调整学习率、batch size等

## 预期效果

### 完全冻结模式
- 训练损失：应该能降到 4-5 左右
- 验证L2误差：可能在 50-70 米
- 训练时间：每个epoch约 5-6 分钟

### 部分微调模式
- 训练损失：应该能降到 3-4 左右
- 验证L2误差：可能在 30-50 米（如果数据质量好）
- 训练时间：每个epoch约 6-7 分钟

## 注意事项

1. **显存占用**
   - 完全冻结：~8-10GB
   - 部分微调：~12-16GB

2. **训练稳定性**
   - 完全冻结模式更稳定，不容易出现梯度爆炸
   - 部分微调需要仔细调整学习率

3. **模型保存**
   - 完全冻结：保存到 `teacher_training_frozen/`
   - 部分微调：保存到 `teacher_training/`
